#!/usr/bin/env python3
"""
Knowledge Base MCP Server
An MCP server that connects to PDF knowledge bases and Obsidian for intelligent note creation.
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Sequence

from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
    LoggingLevel
)

from config import Config
from integrations.pdf_integration import PDFIntegration
from integrations.obsidian_integration import ObsidianIntegration
from models.knowledge_models import KnowledgeExtraction, NoteGenerationRequest

# Configure logging - don't interfere with MCP protocol
import os
if os.getenv("KB_DEBUG", "false").lower() == "true":
    logging.basicConfig(level=logging.INFO)
else:
    # For MCP mode, use ERROR level to minimize output
    logging.basicConfig(level=logging.ERROR)

logger = logging.getLogger("knowledge-base-mcp")

class KnowledgeBaseMCPServer:
    """Main MCP server for knowledge base and Obsidian integration."""
    
    def __init__(self):
        self.server = Server("knowledge-base-mcp")
        self.config = Config()
        
        # Create missing directories first
        created_dirs = self.config.create_missing_directories()
        if created_dirs:
            logger.info(f"Created missing directories: {', '.join(created_dirs)}")
        
        # Validate configuration
        config_errors = self.config.validate_paths()
        if config_errors:
            logger.error("Configuration errors:")
            for error in config_errors:
                logger.error(f"  - {error}")
            raise ValueError("Invalid configuration. Please check your paths.")
        
        # Initialize integrations
        self.pdf_integration = PDFIntegration(self.config)
        self.obsidian_integration = ObsidianIntegration(self.config)
        
        # Register handlers
        self._register_tools()
        self._register_resources()
        
        logger.info("Knowledge Base MCP Server initialized successfully")
    
    def _register_tools(self):
        """Register all available tools."""
        
        @self.server.list_tools()
        async def handle_list_tools() -> List[Tool]:
            """List available tools."""
            return [
                Tool(
                    name="create_note_from_topic",
                    description="Create an Obsidian note about a topic using knowledge from the PDF database",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "topic": {
                                "type": "string",
                                "description": "The topic to create a note about"
                            },
                            "note_type": {
                                "type": "string",
                                "enum": ["general", "summary", "detailed", "flashcards"],
                                "description": "Type of note to create (default: general)"
                            },
                            "focus_areas": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Specific areas to focus on within the topic"
                            },
                            "max_sources": {
                                "type": "integer",
                                "description": "Maximum number of source documents to use (default: 10)"
                            }
                        },
                        "required": ["topic"]
                    }
                ),
                Tool(
                    name="create_note_with_content",
                    description="Create an Obsidian note with LLM-generated content after processing search results",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "Title for the note"
                            },
                            "content": {
                                "type": "string",
                                "description": "The complete markdown content for the note (generated by LLM)"
                            },
                            "tags": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "Tags to apply to the note"
                            },
                            "source_documents": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "List of source documents used in generating the content"
                            },
                            "note_type": {
                                "type": "string",
                                "enum": ["general", "summary", "detailed", "flashcards"],
                                "description": "Type of note (default: general)"
                            }
                        },
                        "required": ["title", "content"]
                    }
                ),
                Tool(
                    name="search_knowledge_base",
                    description="Search the PDF knowledge base for information about a topic",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Search query for the knowledge base"
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "Maximum number of results to return (default: 10)"
                            }
                        },
                        "required": ["query"]
                    }
                ),
                Tool(
                    name="process_pdf_directory",
                    description="Process and index PDFs from a directory into the knowledge base",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "directory_path": {
                                "type": "string",
                                "description": "Path to directory containing PDFs (optional, uses config if not provided)"
                            }
                        }
                    }
                ),
                Tool(
                    name="update_existing_note",
                    description="Update an existing Obsidian note with additional information from the knowledge base",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "note_title": {
                                "type": "string",
                                "description": "Title of the note to update"
                            },
                            "additional_query": {
                                "type": "string",
                                "description": "Additional search query to find more information"
                            }
                        },
                        "required": ["note_title"]
                    }
                ),
                Tool(
                    name="get_study_suggestions",
                    description="Get study suggestions and questions based on knowledge base content",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "topic": {
                                "type": "string",
                                "description": "Topic to generate study suggestions for"
                            },
                            "difficulty_level": {
                                "type": "string",
                                "enum": ["beginner", "intermediate", "advanced"],
                                "description": "Difficulty level for study materials"
                            }
                        },
                        "required": ["topic"]
                    }
                ),
                Tool(
                    name="get_semantic_chunks",
                    description="Get raw semantic chunks for LLM processing - returns full chunk content and metadata",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "Search query for the knowledge base"
                            },
                            "max_results": {
                                "type": "integer",
                                "description": "Maximum number of chunks to return (default: 8)"
                            },
                            "include_metadata": {
                                "type": "boolean",
                                "description": "Include chunk metadata (section titles, page numbers, etc.)"
                            }
                        },
                        "required": ["query"]
                    }
                )
            ]
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: Dict[str, Any]) -> Sequence[TextContent]:
            """Handle tool calls."""
            try:
                if name == "create_note_from_topic":
                    return await self._create_note_from_topic(**arguments)
                elif name == "create_note_with_content":
                    return await self._create_note_with_content(**arguments)
                elif name == "search_knowledge_base":
                    return await self._search_knowledge_base(**arguments)
                elif name == "process_pdf_directory":
                    return await self._process_pdf_directory(**arguments)
                elif name == "update_existing_note":
                    return await self._update_existing_note(**arguments)
                elif name == "get_study_suggestions":
                    return await self._get_study_suggestions(**arguments)
                elif name == "get_semantic_chunks":
                    return await self._get_semantic_chunks(**arguments)
                else:
                    raise ValueError(f"Unknown tool: {name}")
            except Exception as e:
                logger.error(f"Error in tool {name}: {str(e)}")
                return [TextContent(
                    type="text",
                    text=f"Error executing {name}: {str(e)}"
                )]
    
    def _register_resources(self):
        """Register available resources."""
        
        @self.server.list_resources()
        async def handle_list_resources() -> List[Resource]:
            """List available resources."""
            return [
                Resource(
                    uri="kb://database/stats",
                    name="Knowledge Base Statistics",
                    description="Statistics about the PDF knowledge base",
                    mimeType="application/json"
                ),
                Resource(
                    uri="kb://obsidian/stats",
                    name="Obsidian Vault Statistics",
                    description="Statistics about the Obsidian vault",
                    mimeType="application/json"
                ),
                Resource(
                    uri="kb://recent_notes",
                    name="Recent Generated Notes",
                    description="List of recently generated notes",
                    mimeType="application/json"
                )
            ]
        
        @self.server.read_resource()
        async def handle_read_resource(uri: str) -> str:
            """Handle resource reading."""
            try:
                if uri == "kb://database/stats":
                    return await self._get_database_stats_resource()
                elif uri == "kb://obsidian/stats":
                    return await self._get_obsidian_stats_resource()
                elif uri == "kb://recent_notes":
                    return await self._get_recent_notes_resource()
                else:
                    raise ValueError(f"Unknown resource: {uri}")
            except Exception as e:
                logger.error(f"Error reading resource {uri}: {str(e)}")
                return json.dumps({"error": str(e)})
    
    # Tool implementations
    async def _create_note_from_topic(
        self, 
        topic: str, 
        note_type: str = "general",
        focus_areas: List[str] = None,
        max_sources: int = 10
    ) -> Sequence[TextContent]:
        """Create an Obsidian note from knowledge base content."""
        try:
            logger.info(f"Creating note for topic: {topic}")
            
            # Build search query
            search_query = topic
            if focus_areas:
                search_query += " " + " ".join(focus_areas)
            
            # Search knowledge base
            search_results = await self.pdf_integration.search_knowledge_base(
                search_query, 
                max_results=max_sources
            )
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"❌ No relevant information found in knowledge base for topic: '{topic}'"
                )]
            
            # Extract knowledge from search results
            knowledge = await self._extract_knowledge_from_results(topic, search_results, focus_areas)
            
            # Create Obsidian note
            note = await self.obsidian_integration.create_note_from_knowledge(
                topic, knowledge, note_type
            )
            
            # Prepare response
            response_text = (
                f"✅ **Note Created Successfully!**\n\n"
                f"📝 **Title:** {note.title}\n"
                f"📁 **File:** {note.file_path}\n"
                f"🏷️ **Tags:** {', '.join(note.tags)}\n"
                f"📊 **Sources Used:** {len(knowledge.source_documents)}\n"
                f"📄 **Note Type:** {note_type}\n\n"
                f"**Content Preview:**\n"
                f"{note.content[:300]}{'...' if len(note.content) > 300 else ''}\n\n"
                f"**Source Documents:**\n"
            )
            
            for i, source in enumerate(knowledge.source_documents[:5], 1):
                response_text += f"{i}. {source}\n"
            
            if len(knowledge.source_documents) > 5:
                response_text += f"... and {len(knowledge.source_documents) - 5} more sources\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error creating note from topic: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error creating note: {str(e)}"
            )]
    
    async def _create_note_with_content(self, title: str, content: str, tags: List[str] = None, source_documents: List[str] = None, note_type: str = "general") -> Sequence[TextContent]:
        """Create an Obsidian note with LLM-generated content after processing search results."""
        try:
            logger.info(f"Creating note with content for title: {title}")
            
            # Create note
            note = await self.obsidian_integration.create_note_with_content(
                title=title, 
                content=content, 
                tags=tags or [], 
                source_documents=source_documents or [], 
                note_type=note_type
            )
            
            # Prepare response
            sources = source_documents or []
            response_text = (
                f"✅ **Note Created Successfully!**\n\n"
                f"📝 **Title:** {note.title}\n"
                f"📁 **File:** {note.file_path}\n"
                f"🏷️ **Tags:** {', '.join(note.tags)}\n"
                f"📊 **Sources Used:** {len(sources)}\n"
                f"📄 **Note Type:** {note_type}\n\n"
                f"**Content Preview:**\n"
                f"{note.content[:300]}{'...' if len(note.content) > 300 else ''}\n\n"
            )
            
            if sources:
                response_text += f"**Source Documents:**\n"
                for i, source in enumerate(sources[:5], 1):
                    response_text += f"{i}. {source}\n"
                
                if len(sources) > 5:
                    response_text += f"... and {len(sources) - 5} more sources\n"
            else:
                response_text += f"**Source Documents:** None specified\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error creating note with content: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error creating note: {str(e)}"
            )]
    
    async def _search_knowledge_base(self, query: str, max_results: int = 10) -> Sequence[TextContent]:
        """Search the knowledge base and return results."""
        try:
            logger.info(f"Searching knowledge base for: {query}")
            
            search_results = await self.pdf_integration.search_knowledge_base(query, max_results)
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"❌ No results found for query: '{query}'"
                )]
            
            # Format results
            response_text = f"🔍 **Search Results for:** '{query}'\n\n"
            response_text += f"**Found {len(search_results)} relevant results:**\n\n"
            
            for i, result in enumerate(search_results, 1):
                response_text += f"**Result {i}** (Similarity: {result.similarity_score:.2f})\n"
                response_text += f"📄 **Source:** {result.source_document}\n"
                response_text += f"**Content:** {result.chunk.content[:200]}...\n\n"
                response_text += "---\n\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error searching knowledge base: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error searching knowledge base: {str(e)}"
            )]
    
    async def _process_pdf_directory(self, directory_path: str = None) -> Sequence[TextContent]:
        """Process PDFs from a directory."""
        try:
            if directory_path:
                directories = [directory_path]
            else:
                directories = self.config.pdf.pdf_directories
            
            total_processed = 0
            results = []
            
            for directory in directories:
                logger.info(f"Processing PDF directory: {directory}")
                docs = await self.pdf_integration.process_pdf_directory(directory)
                total_processed += len(docs)
                results.extend(docs)
            
            response_text = (
                f"✅ **PDF Processing Complete!**\n\n"
                f"📁 **Directories Processed:** {len(directories)}\n"
                f"📄 **Documents Processed:** {total_processed}\n"
                f"🔢 **Total Chunks Created:** {sum(1 for doc in results if doc.embedding_generated)}\n\n"
                f"**Processed Documents:**\n"
            )
            
            for doc in results[:10]:  # Show first 10
                response_text += f"- {doc.title} ({doc.page_count} pages)\n"
            
            if len(results) > 10:
                response_text += f"... and {len(results) - 10} more documents\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error processing PDF directory: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error processing PDFs: {str(e)}"
            )]
    
    async def _update_existing_note(self, note_title: str, additional_query: str = None) -> Sequence[TextContent]:
        """Update an existing Obsidian note with more information."""
        try:
            # Search for existing note
            existing_notes = await self.obsidian_integration.search_notes(note_title)
            
            if not existing_notes:
                return [TextContent(
                    type="text",
                    text=f"❌ No note found with title: '{note_title}'"
                )]
            
            note = existing_notes[0]  # Take first match
            
            # Search for additional information
            search_query = additional_query or note.topic or note_title
            search_results = await self.pdf_integration.search_knowledge_base(search_query)
            
            if search_results:
                # Extract new knowledge
                knowledge = await self._extract_knowledge_from_results(
                    note.topic or note_title, 
                    search_results
                )
                
                # Add new content to note
                additional_content = f"\n\n## Additional Information (Updated {datetime.now().strftime('%Y-%m-%d')})\n\n"
                additional_content += knowledge.summary + "\n\n"
                
                if knowledge.key_points:
                    additional_content += "### New Key Points\n\n"
                    for point in knowledge.key_points:
                        additional_content += f"- {point}\n"
                
                # Update note
                new_content = note.content + additional_content
                await self.obsidian_integration.update_note(note, new_content=new_content)
                
                response_text = (
                    f"✅ **Note Updated Successfully!**\n\n"
                    f"📝 **Note:** {note.title}\n"
                    f"📁 **File:** {note.file_path}\n"
                    f"📊 **New Sources:** {len(knowledge.source_documents)}\n"
                    f"➕ **Content Added:** {len(additional_content)} characters\n"
                )
            else:
                response_text = f"❌ No additional information found for query: '{search_query}'"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error updating note: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error updating note: {str(e)}"
            )]
    
    async def _get_study_suggestions(self, topic: str, difficulty_level: str = "intermediate") -> Sequence[TextContent]:
        """Generate study suggestions based on knowledge base content."""
        try:
            # Search for topic information
            search_results = await self.pdf_integration.search_knowledge_base(topic)
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"❌ No information found for topic: '{topic}'"
                )]
            
            # Extract knowledge
            knowledge = await self._extract_knowledge_from_results(topic, search_results)
            
            # Generate study suggestions based on difficulty level
            response_text = f"📚 **Study Suggestions for: {topic}**\n\n"
            response_text += f"**Difficulty Level:** {difficulty_level.title()}\n\n"
            
            # Study questions
            if knowledge.questions:
                response_text += "## 🤔 Study Questions\n\n"
                for i, question in enumerate(knowledge.questions[:5], 1):
                    response_text += f"{i}. {question}\n"
                response_text += "\n"
            
            # Flashcards
            if knowledge.flashcards:
                response_text += "## 🃏 Flashcard Suggestions\n\n"
                for i, card in enumerate(knowledge.flashcards[:3], 1):
                    front = card.get('front', card.get('question', ''))
                    back = card.get('back', card.get('answer', ''))
                    response_text += f"**Card {i}:**\n"
                    response_text += f"*Front:* {front}\n"
                    response_text += f"*Back:* {back}\n\n"
            
            # Key areas to focus on
            if knowledge.key_points:
                response_text += "## 🎯 Key Areas to Focus On\n\n"
                for point in knowledge.key_points[:5]:
                    response_text += f"- {point}\n"
                response_text += "\n"
            
            # Related topics to explore
            if knowledge.related_concepts:
                response_text += "## 🔗 Related Topics to Explore\n\n"
                for concept in knowledge.related_concepts[:5]:
                    response_text += f"- {concept}\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error generating study suggestions: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error generating study suggestions: {str(e)}"
            )]
    
    async def _get_semantic_chunks(self, query: str, max_results: int = 8, include_metadata: bool = True) -> Sequence[TextContent]:
        """Get raw semantic chunks for LLM processing - returns full chunk content and metadata."""
        try:
            logger.info(f"Getting semantic chunks for query: {query}")
            
            search_results = await self.pdf_integration.search_knowledge_base(query, max_results)
            
            if not search_results:
                return [TextContent(
                    type="text",
                    text=f"❌ No results found for query: '{query}'"
                )]
            
            # Format complete chunks for LLM processing
            response_text = f"🧠 **SEMANTIC CHUNKS FOR LLM PROCESSING**\n"
            response_text += f"**Query:** '{query}'\n"
            response_text += f"**Found:** {len(search_results)} high-quality semantic chunks\n\n"
            response_text += "=" * 80 + "\n\n"
            
            for i, result in enumerate(search_results, 1):
                chunk = result.chunk
                source_file = result.source_document.split('/')[-1]  # Get filename only
                
                response_text += f"**CHUNK {i}/{len(search_results)}**\n"
                response_text += f"📄 **Source:** {source_file}\n"
                response_text += f"🎯 **Similarity Score:** {result.similarity_score:.3f}\n"
                response_text += f"🔤 **Chunk Type:** {chunk.chunk_type}\n"
                response_text += f"📊 **Word Count:** {len(chunk.content.split())} words\n"
                
                # Include rich metadata if available
                if include_metadata and hasattr(chunk, 'metadata') and chunk.metadata:
                    response_text += f"📋 **Metadata:**\n"
                    for key, value in chunk.metadata.items():
                        if value is not None:
                            response_text += f"   • {key.replace('_', ' ').title()}: {value}\n"
                
                response_text += f"\n**FULL CONTENT:**\n"
                response_text += f"```\n{chunk.content}\n```\n\n"
                response_text += "=" * 80 + "\n\n"
            
            response_text += f"📚 **PROCESSING INSTRUCTIONS FOR LLM:**\n"
            response_text += f"• These {len(search_results)} chunks contain semantically preserved content\n"
            response_text += f"• Each chunk maintains paragraph boundaries and context\n"
            response_text += f"• Metadata provides section context and page attribution\n"
            response_text += f"• Process these chunks to create comprehensive notes\n"
            response_text += f"• Use the metadata for proper citations\n\n"
            
            response_text += f"**SOURCE DOCUMENTS:**\n"
            unique_sources = list(set(result.source_document.split('/')[-1] for result in search_results))
            for source in unique_sources:
                response_text += f"• {source}\n"
            
            return [TextContent(type="text", text=response_text)]
            
        except Exception as e:
            logger.error(f"Error getting semantic chunks: {e}")
            return [TextContent(
                type="text",
                text=f"❌ Error getting semantic chunks: {str(e)}"
            )]
    
    async def _extract_knowledge_from_results(
        self, 
        topic: str, 
        search_results: List,
        focus_areas: List[str] = None
    ) -> KnowledgeExtraction:
        """Extract structured knowledge from search results."""
        
        # Combine content from search results
        combined_content = ""
        source_documents = []
        citations = []
        
        for result in search_results:
            combined_content += result.chunk.content + "\n\n"
            if result.source_document not in source_documents:
                source_documents.append(result.source_document)
            
            citations.append({
                "source": result.source_document,
                "similarity": result.similarity_score,
                "content_preview": result.chunk.content[:100]
            })
        
        # Extract key information (this is a simplified version)
        # In a real implementation, you might use an LLM to analyze the content
        
        # Generate summary (take first part of combined content)
        summary = combined_content[:500] + "..." if len(combined_content) > 500 else combined_content
        
        # Extract key points (simplified - split by paragraphs and take significant ones)
        paragraphs = [p.strip() for p in combined_content.split('\n\n') if len(p.strip()) > 50]
        key_points = paragraphs[:5]  # Take first 5 significant paragraphs
        
        # Generate related concepts (extract capitalized terms)
        import re
        concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', combined_content)
        related_concepts = list(set(concepts))[:10]  # Unique concepts, limit to 10
        
        # Generate simple questions
        questions = [
            f"What is {topic}?",
            f"How does {topic} work?",
            f"What are the key components of {topic}?",
            f"What are the applications of {topic}?",
            f"What are the advantages and disadvantages of {topic}?"
        ]
        
        # Generate basic flashcards
        flashcards = []
        for i, point in enumerate(key_points[:3]):
            flashcards.append({
                "front": f"What is a key point about {topic}?",
                "back": point[:200]
            })
        
        return KnowledgeExtraction(
            topic=topic,
            summary=summary,
            key_points=key_points,
            related_concepts=related_concepts,
            source_documents=source_documents,
            citations=citations,
            flashcards=flashcards,
            questions=questions
        )
    
    # Resource implementations
    async def _get_database_stats_resource(self) -> str:
        """Get knowledge base statistics as JSON resource."""
        try:
            stats = self.pdf_integration.get_database_stats()
            return json.dumps(stats, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)})
    
    async def _get_obsidian_stats_resource(self) -> str:
        """Get Obsidian vault statistics as JSON resource."""
        try:
            stats = await self.obsidian_integration.get_vault_stats()
            return json.dumps(stats, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)})
    
    async def _get_recent_notes_resource(self) -> str:
        """Get recent generated notes as JSON resource."""
        try:
            # Search for KB generated notes
            kb_notes = await self.obsidian_integration.search_notes("kb_generated:true")
            
            # Sort by creation date and take recent ones
            recent_notes = sorted(
                [note for note in kb_notes if note.created_date],
                key=lambda x: x.created_date,
                reverse=True
            )[:10]
            
            notes_data = [
                {
                    "title": note.title,
                    "topic": note.topic,
                    "created_date": note.created_date.isoformat() if note.created_date else None,
                    "tags": note.tags,
                    "source_count": len(note.source_documents)
                }
                for note in recent_notes
            ]
            
            return json.dumps({
                "recent_notes": notes_data,
                "total_count": len(notes_data)
            }, indent=2)
            
        except Exception as e:
            return json.dumps({"error": str(e)})


async def main():
    """Main entry point for the MCP server."""
    try:
        knowledge_server = KnowledgeBaseMCPServer()
        
        async with stdio_server() as (read_stream, write_stream):
            await knowledge_server.server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="knowledge-base-mcp",
                    server_version="1.0.0",
                    capabilities={
                        "tools": {},
                        "resources": {}
                    }
                )
            )
    except Exception as e:
        logger.error(f"Failed to start server: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 